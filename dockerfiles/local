# syntax = docker/dockerfile:experimental
FROM nvcr.io/nvidia/tensorrt:22.11-py3

ENV DEBIAN_FRONTEND=noninteractive

# Install latest pip and TensorRT
RUN --mount=type=cache,mode=0755,target=/root/.cache/pip pip install --upgrade pip

# Clone the TensorRT repository
RUN git clone https://github.com/NVIDIA/TensorRT.git --single-branch \
    && cd TensorRT/ \
    && git submodule update --init --recursive

# Build TensorRT plugins
ENV TRT_OSSPATH=/workspace/TensorRT
WORKDIR /workspace/TensorRT
RUN mkdir -p build \
    && cd build \
    && cmake .. -DTRT_OUT_DIR=$PWD/out \
    && cd plugin \
    && make -j$(nproc)

# Set environment variables
ENV LD_PRELOAD_PATH="${TRT_OSSPATH}:${LD_PRELOAD_PATH}"
ENV CUDA_MODULE_LOADING=LAZY

# Prepare the environment
WORKDIR /workspace/voltaML-fast-stable-diffusion

# Install python dependencies (multi-stage build to enable caching)
COPY requirements/pytorch.txt requirements/pytorch.txt
RUN --mount=type=cache,mode=0755,target=/root/.cache/pip pip3 install -r requirements/pytorch.txt

COPY requirements/api.txt requirements/api.txt
RUN --mount=type=cache,mode=0755,target=/root/.cache/pip pip3 install -r requirements/api.txt

COPY requirements/tensorrt.txt requirements/tensorrt.txt
RUN --mount=type=cache,mode=0755,target=/root/.cache/pip pip3 install -r requirements/tensorrt.txt

# Copy the source code
COPY . /workspace/voltaML-fast-stable-diffusion

# Set environment variables
ENV LOG_LEVEL=INFO

# Run the server
RUN chmod +x scripts/start.sh
ENTRYPOINT ["bash", "./scripts/start.sh"]
